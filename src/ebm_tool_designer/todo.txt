TO DO


1. Make a separate file for training the reward model called train_reward_model.py, rather than training being in the main func of reward_model.py

2. Get EBM working and LD sampling

Note that there is an issue with taking the gradient of a uniform prior in the energy function. Here, we must reparameterise into an unconstrained space 
before calculating the energy and then coverting back. e.g. partial deriv E w.r.t. phi not tau.

We can also implement metropolis hastings corrections to improve convergence. If you skip the MH step, this is technically "Unadjusted Langevin" in phi-space

3. Look into integrating into Matteos codebase via collecting an offline dataset







# tests: 
# 1. get two tool vectors and work out the joint energy of each
# 2. check energy is higher when reward pred is further from target
# 3. 


# CONSIDERATIONS:

# The $\sin(\theta)^2 + \cos(\theta)^2 = 1$ Constraint. sample 3: $(l_1, l_2, \theta)$, This guarantees the angle is always valid.

# the reward model was trained on normalised data: apply feature_stats to tau and c inside the joint energy function - 
# If your reward model was trained on normalized data, you should be performing Langevin in the normalized space.

# MALA combines Langevin Dynamics with a Metropolis-Hastings acceptance step. 


"""
        
        # 1. INITIALISE tool params, tau, by sampling from the prior (e.g., [batch_size, 4])
        
        l1_init,l2_init,theta_init = self.prior.sample(batch_size)
        
        tau = torch.stack([l1_init, l2_init, theta_init], dim=-1).to(self.device).requires_grad_(True)
        
        c_target = c_target.to(self.device).detach()
        
        # 2. TRANSFORM tau by converting into unconstrained space (phi) using sigmoid
        
        phi = self.transform_to_phi(tau)

        for i in range(self.n_sampling_steps):
            
            # 3. ENERGY CALCULATION
        
            # Convert phi back to physical tau inside the gradient tape
            current_tau, log_jacob = self.transform_to_physical_space(phi)
            
            # Calculate energy using your existing joint_energy logic
            # slice tau to get individual components for the Energy logic (which works on theta not sin(theta))
            l1_current = current_tau[:, 0:1]
            l2_current = current_tau[:, 1:2]
            theta_current = current_tau[:, 2:3]
            
            energy = self.joint_energy(l1_current,l2_current,theta_current, log_jacob, c_target, r_target)
            
            # Total MALA Energy = Physical Energy - Log Determinant
            total_E = energy.sum()
            
            # 4. GRADIENT STEP
            grad = torch.autograd.grad(total_E, phi)[0]
            
            # 5. PROPOSAL (Langevin Step)
            noise = torch.randn_like(phi)
            phi_prop = phi - self.eta * grad + torch.sqrt(torch.tensor(2 * self.eta)) * noise
            
            # can perform Metropolis-Hastings Accept/Reject step here
            if mc_corrected:
                pass
            
            phi = phi_prop.detach().requires_grad_(True)

        # 6. FINAL OUTPUT: Convert the last phi back to physical tau
        with torch.no_grad():
            final_tau, _ = self.transform_to_physical_space(phi)
            
        return final_tau
                
"""